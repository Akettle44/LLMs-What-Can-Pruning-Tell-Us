{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf7b83d-6920-43ea-a431-924cb0614144",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 24pt;\"><strong>Data Manipulation</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d37043-1227-4fd9-a919-b5f489835308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, BertOnlyNSPHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8656ea-98dc-469d-9843-846b1c341e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/jovyan/pub/en-fr.csv'\n",
    "enfr_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa08cdf8-f532-4418-be86-6467dd192f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataframe:\n",
      "(22520376, 2)\n",
      "Cardinality of categorical features in the dataframe:\n",
      "en    20317468\n",
      "fr    21358854\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "enfr_df['en'] = enfr_df['en'].astype(str).fillna('')\n",
    "enfr_df['fr'] = enfr_df['fr'].astype(str).fillna('')\n",
    "# Dimension\n",
    "print(\"Dimensions of the dataframe:\")\n",
    "print(enfr_df.shape)\n",
    "# Cardinality\n",
    "print(\"Cardinality of categorical features in the dataframe:\")\n",
    "print(enfr_df.select_dtypes(include=\"object\").nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bebc995-8de8-451b-a86b-793ec3f4176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_enfr_df = enfr_df.sample(n=18949, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ae58716-08cd-4a12-a6e1-1ae54e2de2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_path = '/home/jovyan/pub/rt_reviews.csv'\n",
    "rt_df = pd.read_csv(rt_path, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a9256fb-bc09-4992-aa2a-7fc256a0be05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2027767/224889234.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  rt_df['Freshness'] = rt_df['Freshness'].replace({'fresh': 0, 'rotten': 1})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataframe:\n",
      "(480000, 2)\n",
      "Duplicates in the dataframe: 140284 (29.23%)\n",
      "Cardinality of categorical features in the dataframe:\n",
      "Review    339697\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "rt_df['Review'] = rt_df['Review'].astype(str).fillna('')\n",
    "rt_df['Freshness'] = rt_df['Freshness'].replace({'fresh': 0, 'rotten': 1})\n",
    "# Dimension\n",
    "print(\"Dimensions of the dataframe:\")\n",
    "print(rt_df.shape)\n",
    "# Duplicates \n",
    "duplicates_count = rt_df.duplicated().sum()\n",
    "duplicates_percentage = 100 * duplicates_count / len(rt_df)\n",
    "print(f\"Duplicates in the dataframe: {duplicates_count} ({duplicates_percentage:.2f}%)\")\n",
    "# Cardinality \n",
    "print(\"Cardinality of categorical features in the dataframe:\")\n",
    "print(rt_df.select_dtypes(include=\"object\").nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7230b5ae-7aed-4222-8d66-45fa0b548049",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_rt_df = rt_df.sample(n=18949, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4483cc5e-b0fd-494c-8739-6125a5a651ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/jovyan/pub/bs_train.parquet'\n",
    "summ_df = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "349b4e00-e9b4-4af8-900e-8a90f116c2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataframe:\n",
      "(18949, 3)\n",
      "Duplicates in the dataframe: 0 (0.00%)\n",
      "Cardinality of categorical features in the dataframe:\n",
      "text       18941\n",
      "summary    18949\n",
      "title      17106\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "summ_df['text'] = summ_df['text'].astype(str).fillna('')\n",
    "summ_df['summary'] = summ_df['summary'].astype(str).fillna('')\n",
    "summ_df['title'] = summ_df['title'].astype(str).fillna('')\n",
    "# Dimension\n",
    "print(\"Dimensions of the dataframe:\")\n",
    "print(summ_df.shape)\n",
    "# Duplicates \n",
    "duplicates_count = summ_df.duplicated().sum()\n",
    "duplicates_percentage = 100 * duplicates_count / len(summ_df)\n",
    "print(f\"Duplicates in the dataframe: {duplicates_count} ({duplicates_percentage:.2f}%)\")\n",
    "# Cardinality (for categorical features)\n",
    "print(\"Cardinality of categorical features in the dataframe:\")\n",
    "print(summ_df.select_dtypes(include=\"object\").nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40a57f44-faaa-465f-82d1-97d1185a2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_summ_df = summ_df.sample(n=18949, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "027503c4-dbe7-404b-9812-21a7e4e2328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('/home/jovyan/pub/mcq.json', lines=True)\n",
    "single_choice_data = data[data['choice_type'] == 'single']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c5d16a7-cb1a-425c-a685-8c35ea88fdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2027767/2684607933.py:4: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['3' '3' '2' ... '2' '1' '1']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  single_choice_data.loc[:, 'cop'] = single_choice_data['cop'].astype(str).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataframe:\n",
      "(120765, 9)\n",
      "Duplicates in the dataframe: 0 (0.00%)\n",
      "Cardinality of categorical features in the dataframe:\n",
      "question        120765\n",
      "exp             103814\n",
      "cop                  4\n",
      "opa              51427\n",
      "opb              53743\n",
      "opc              54864\n",
      "opd              56004\n",
      "subject_name        21\n",
      "topic_name        2307\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# dropping Nan rows \n",
    "single_choice_data.loc[:, 'question'] = single_choice_data['question'].astype(str).fillna('')\n",
    "single_choice_data.loc[:, 'exp'] = single_choice_data['exp'].astype(str).fillna('')\n",
    "single_choice_data.loc[:, 'cop'] = single_choice_data['cop'].astype(str).fillna('')\n",
    "single_choice_data.loc[:, 'opa'] = single_choice_data['opa'].astype(str).fillna('')\n",
    "single_choice_data.loc[:, 'opb'] = single_choice_data['opb'].astype(str).fillna('')\n",
    "single_choice_data.loc[:, 'opc'] = single_choice_data['opc'].astype(str).fillna('')\n",
    "single_choice_data.loc[:, 'opd'] = single_choice_data['opd'].astype(str).fillna('')\n",
    "single_choice_data.loc[:, 'subject_name'] = single_choice_data['subject_name'].astype(str).fillna('')\n",
    "single_choice_data.loc[:, 'topic_name'] = single_choice_data['topic_name'].astype(str).fillna('')\n",
    "# dropping redundant cols\n",
    "single_choice_data = single_choice_data.drop(columns=['choice_type'])\n",
    "single_choice_data = single_choice_data.drop(columns=['id'])\n",
    "# Basic Sanity checks\n",
    "# Dimension\n",
    "print(\"Dimensions of the dataframe:\")\n",
    "print(single_choice_data.shape)\n",
    "# Duplicates \n",
    "duplicates_count = single_choice_data.duplicated().sum()\n",
    "duplicates_percentage = 100 * duplicates_count / len(single_choice_data)\n",
    "print(f\"Duplicates in the dataframe: {duplicates_count} ({duplicates_percentage:.2f}%)\")\n",
    "# Cardinality (for categorical features)\n",
    "print(\"Cardinality of categorical features in the dataframe:\")\n",
    "print(single_choice_data.select_dtypes(include=\"object\").nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc3b00f4-d0a1-4845-a1df-1db4a6601651",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_single_choice_data = single_choice_data.sample(n=18949, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa609e8-e9ba-487a-a766-e2c81bd7956d",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 24pt;\"><strong>Creating DataLoaders</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8419e62e-65c5-4f88-baad-8df6d50793bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation Data Sizes\n",
      "Input IDs: torch.Size([18949, 512])\n",
      "Attention Mask: torch.Size([18949, 512])\n",
      "Labels: torch.Size([18949, 512])\n",
      "\n",
      "Sentiment Classification Data Sizes\n",
      "Input IDs: torch.Size([18949, 116])\n",
      "Attention Mask: torch.Size([18949, 116])\n",
      "Labels: torch.Size([18949])\n",
      "\n",
      "Summarization Data Sizes\n",
      "Text Input IDs: torch.Size([18949, 512])\n",
      "Text Attention Mask: torch.Size([18949, 512])\n",
      "Summary Labels: torch.Size([18949, 512])\n",
      "\n",
      "Multiple Choice Data Sizes\n",
      "Question Input IDs: torch.Size([18949, 278])\n",
      "Question Attention Mask: torch.Size([18949, 278])\n",
      "Labels: torch.Size([18949])\n",
      "\n",
      "Translation DataLoader Inspection:\n",
      "Input IDs size: torch.Size([8, 512])\n",
      "Attention Mask size: torch.Size([8, 512])\n",
      "Labels size: torch.Size([8, 512])\n",
      "\n",
      "Sentiment Classification DataLoader Inspection:\n",
      "Input IDs size: torch.Size([8, 116])\n",
      "Attention Mask size: torch.Size([8, 116])\n",
      "Labels size: torch.Size([8])\n",
      "\n",
      "Summarization DataLoader Inspection:\n",
      "Input IDs size: torch.Size([8, 512])\n",
      "Attention Mask size: torch.Size([8, 512])\n",
      "Labels size: torch.Size([8, 512])\n",
      "\n",
      "Multiple Choice DataLoader Inspection:\n",
      "Input IDs size: torch.Size([8, 278])\n",
      "Attention Mask size: torch.Size([8, 278])\n",
      "Labels size: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Sampling small subset of the dataset\n",
    "sm_enfr_df = enfr_df.sample(n=18949, random_state=42)\n",
    "sm_rt_df = rt_df.sample(n=18949, random_state=42)\n",
    "sm_summ_df = summ_df.sample(n=18949, random_state=42) \n",
    "sm_mcq_df = single_choice_data.sample(n=18949, random_state=42) \n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Trranslation dataset\n",
    "enfr_inputs = tokenizer(sm_enfr_df['en'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "enfr_labels = tokenizer(sm_enfr_df['fr'].tolist(), return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "print(\"Translation Data Sizes\")\n",
    "print(f\"Input IDs: {enfr_inputs['input_ids'].size()}\")\n",
    "print(f\"Attention Mask: {enfr_inputs['attention_mask'].size()}\")\n",
    "print(f\"Labels: {enfr_labels.size()}\")\n",
    "translation_dataset = TensorDataset(enfr_inputs['input_ids'], enfr_inputs['attention_mask'], enfr_labels)\n",
    "translation_dataloader = DataLoader(translation_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Sentiment classification dataset\n",
    "rt_inputs = tokenizer(sm_rt_df['Review'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "rt_labels = torch.tensor([1 if label == 'fresh' else 0 for label in sm_rt_df['Freshness'].tolist()])\n",
    "print(\"\\nSentiment Classification Data Sizes\")\n",
    "print(f\"Input IDs: {rt_inputs['input_ids'].size()}\")\n",
    "print(f\"Attention Mask: {rt_inputs['attention_mask'].size()}\")\n",
    "print(f\"Labels: {rt_labels.size()}\")\n",
    "classification_dataset = TensorDataset(rt_inputs['input_ids'], rt_inputs['attention_mask'], rt_labels)\n",
    "classification_dataloader = DataLoader(classification_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Text Summarization dataset\n",
    "text_inputs = tokenizer(sm_summ_df['text'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "summary_labels = tokenizer(sm_summ_df['summary'].tolist(), return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "print(\"\\nSummarization Data Sizes\")\n",
    "print(f\"Text Input IDs: {text_inputs['input_ids'].size()}\")\n",
    "print(f\"Text Attention Mask: {text_inputs['attention_mask'].size()}\")\n",
    "print(f\"Summary Labels: {summary_labels.size()}\")\n",
    "summarization_dataset = TensorDataset(text_inputs['input_ids'], text_inputs['attention_mask'], summary_labels)\n",
    "summarization_dataloader = DataLoader(summarization_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Multiple choice dataset\n",
    "mcq_inputs = tokenizer(sm_mcq_df['question'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "mcq_labels = torch.tensor([1 if label == 'correct' else 0 for label in sm_mcq_df['cop'].tolist()])\n",
    "print(\"\\nMultiple Choice Data Sizes\")\n",
    "print(f\"Question Input IDs: {mcq_inputs['input_ids'].size()}\")\n",
    "print(f\"Question Attention Mask: {mcq_inputs['attention_mask'].size()}\")\n",
    "print(f\"Labels: {mcq_labels.size()}\")\n",
    "mcq_dataset = TensorDataset(mcq_inputs['input_ids'], mcq_inputs['attention_mask'], mcq_labels)\n",
    "mcq_dataloader = DataLoader(mcq_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Inspecting the DataLoaders: 1 batch\n",
    "def inspect_dataloader(dataloader, name):\n",
    "    print(f\"\\n{name} DataLoader Inspection:\")\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        print(f\"Input IDs size: {input_ids.size()}\")\n",
    "        print(f\"Attention Mask size: {attention_mask.size()}\")\n",
    "        print(f\"Labels size: {labels.size()}\")\n",
    "        break  \n",
    "\n",
    "inspect_dataloader(translation_dataloader, \"Translation\")\n",
    "inspect_dataloader(classification_dataloader, \"Sentiment Classification\")\n",
    "inspect_dataloader(summarization_dataloader, \"Summarization\")\n",
    "inspect_dataloader(mcq_dataloader, \"Multiple Choice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7001e862-1ad6-40f8-ac90-b4a54962e44c",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 24pt;\"><strong>Defining `BertCustomHead` and training</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0fc59b5-4c29-4edc-9100-4581646758c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BertOnlyNSPHead' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     84\u001b[0m config \u001b[38;5;241m=\u001b[39m BertConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertCustomHead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msequence_classification\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m)\n\u001b[1;32m     88\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m'\u001b[39m: translation_dataloader,\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m: classification_dataloader,\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m'\u001b[39m: summarization_dataloader,\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiple_choice\u001b[39m\u001b[38;5;124m'\u001b[39m: mcq_dataloader\n\u001b[1;32m     93\u001b[0m }\n",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m, in \u001b[0;36mBertCustomHead.__init__\u001b[0;34m(self, config, num_classes, task_type)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert \u001b[38;5;241m=\u001b[39m BertModel(config)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_type \u001b[38;5;241m=\u001b[39m task_type\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence_classification\u001b[39m\u001b[38;5;124m'\u001b[39m: nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, num_classes),\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_classification\u001b[39m\u001b[38;5;124m'\u001b[39m: nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, num_classes),\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiple_choice\u001b[39m\u001b[38;5;124m'\u001b[39m: nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_sentence_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mBertOnlyNSPHead\u001b[49m(config)  \n\u001b[1;32m     16\u001b[0m }\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fns \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence_classification\u001b[39m\u001b[38;5;124m'\u001b[39m: nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(),\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_classification\u001b[39m\u001b[38;5;124m'\u001b[39m: nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(),\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiple_choice\u001b[39m\u001b[38;5;124m'\u001b[39m: nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss(),\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_sentence_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m: nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertOnlyNSPHead' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class BertCustomHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the BertCustomHead module. \n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_classes, task_type='sequence_classification'):\n",
    "        super(BertCustomHead, self).__init__()\n",
    "        self.bert = BertModel(config)\n",
    "        self.task_type = task_type\n",
    "        self.heads = {\n",
    "            'sequence_classification': nn.Linear(config.hidden_size, num_classes),\n",
    "            'token_classification': nn.Linear(config.hidden_size, num_classes),\n",
    "            'multiple_choice': nn.Linear(config.hidden_size, 1),\n",
    "            'next_sentence_prediction': BertOnlyNSPHead(config)  \n",
    "        }\n",
    "        self.loss_fns = {\n",
    "            'sequence_classification': nn.CrossEntropyLoss(),\n",
    "            'token_classification': nn.CrossEntropyLoss(),\n",
    "            'multiple_choice': nn.BCEWithLogitsLoss(),\n",
    "            'next_sentence_prediction': nn.CrossEntropyLoss()\n",
    "        }\n",
    "        \n",
    "        if task_type not in self.heads:\n",
    "            raise ValueError(\"Invalid task type. Supported types: 'sequence_classification', 'token_classification', 'multiple_choice', 'next_sentence_prediction'\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, next_sentence_labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.heads[self.task_type](pooled_output)\n",
    "\n",
    "def train(model, dataloaders, optimizer, num_epochs=5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Training loop for `num_epochs` epochs.\n",
    "    \n",
    "    Example usage:\n",
    "    \n",
    "            config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "            model = BertCustomHead(config, num_classes=2, task_type='sequence_classification')\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "            dataloaders = {\n",
    "                'translation': translation_dataloader,\n",
    "            }\n",
    "\n",
    "            train(model, dataloaders, optimizer, num_epochs=5, device='cuda')\n",
    "\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    task_losses = {task_name: [] for task_name in dataloaders.keys()}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for name, dataloader in dataloaders.items():\n",
    "            for batch in dataloader:\n",
    "                # Move batch to device\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, attention_mask, labels = batch\n",
    "\n",
    "                # Determine the task type based on the name of the data loader\n",
    "                if name == 'translation':\n",
    "                    task_type = 'sequence_classification'\n",
    "                    labels = labels.view(-1)  # Flatten the labels for sequence classification\n",
    "                elif name == 'classification':\n",
    "                    task_type = 'sequence_classification'\n",
    "                elif name == 'summarization':\n",
    "                    task_type = 'token_classification'\n",
    "                elif name == 'multiple_choice':\n",
    "                    task_type = 'multiple_choice'\n",
    "                    # Ensure labels have the same batch size as logits\n",
    "                    labels = labels.unsqueeze(1).float()  # Add a dimension to match logits\n",
    "                    labels = labels.expand(-1, logits.size(1))  # Expand labels to match logits shape\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid data loader name. Supported names: 'translation', 'classification', 'summarization', 'multiple_choice'\")    \n",
    "                # Forward pass\n",
    "                logits = model(input_ids, attention_mask)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = model.loss_fns[task_type](logits, labels)\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                task_losses[name].append(loss.item())\n",
    "                \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Total Loss: {total_loss}\")\n",
    "        for task_name, losses in task_losses.items():\n",
    "            print(f\"Task: {task_name}, Last Loss: {losses[-1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a8857d-50f1-4e40-898b-4360c58c17bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
