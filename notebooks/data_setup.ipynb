{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf7b83d-6920-43ea-a431-924cb0614144",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 24pt;\"><strong>Data Manipulation</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d37043-1227-4fd9-a919-b5f489835308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertConfig, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa609e8-e9ba-487a-a766-e2c81bd7956d",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 24pt;\"><strong>Creating DataLoaders</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8419e62e-65c5-4f88-baad-8df6d50793bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment Classification Data Sizes\n",
      "Input IDs: torch.Size([23455, 116])\n",
      "Attention Mask: torch.Size([23455, 116])\n",
      "Labels: torch.Size([23455])\n",
      "\n",
      "Sentiment Classification DataLoader Inspection:\n",
      "Input IDs size: torch.Size([8, 116])\n",
      "Attention Mask size: torch.Size([8, 116])\n",
      "Labels size: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "sm_enfr_df = pd.read_csv('/home/jovyan/pub/bert_pruning/transl_data.csv')\n",
    "sm_rt_df = pd.read_csv('/home/jovyan/pub/bert_pruning/rt_data.csv')\n",
    "sm_summ_df = pd.read_csv('/home/jovyan/pub/bert_pruning/summ_data.csv')\n",
    "sm_mcq_df = pd.read_csv('/home/jovyan/pub/bert_pruning/mc_data.csv')\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\"\"\"\n",
    "# Trranslation dataset\n",
    "enfr_inputs = tokenizer(sm_enfr_df['en'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "enfr_labels = tokenizer(sm_enfr_df['fr'].tolist(), return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "print(\"Translation Data Sizes\")\n",
    "print(f\"Input IDs: {enfr_inputs['input_ids'].size()}\")\n",
    "print(f\"Attention Mask: {enfr_inputs['attention_mask'].size()}\")\n",
    "print(f\"Labels: {enfr_labels.size()}\")\n",
    "translation_dataset = TensorDataset(enfr_inputs['input_ids'], enfr_inputs['attention_mask'], enfr_labels)\n",
    "translation_dataloader = DataLoader(translation_dataset, batch_size=4096, shuffle=True)\n",
    "\"\"\"\n",
    "\n",
    "# Sentiment classification dataset\n",
    "rt_inputs = tokenizer(sm_rt_df['Review'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "rt_labels = torch.tensor([1 if label == 'fresh' else 0 for label in sm_rt_df['Freshness'].tolist()])\n",
    "print(\"\\nSentiment Classification Data Sizes\")\n",
    "print(f\"Input IDs: {rt_inputs['input_ids'].size()}\")\n",
    "print(f\"Attention Mask: {rt_inputs['attention_mask'].size()}\")\n",
    "print(f\"Labels: {rt_labels.size()}\")\n",
    "classification_dataset = TensorDataset(rt_inputs['input_ids'], rt_inputs['attention_mask'], rt_labels)\n",
    "classification_dataloader = DataLoader(classification_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\"\"\"\n",
    "# Text Summarization dataset\n",
    "text_inputs = tokenizer(sm_summ_df['text'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "summary_labels = tokenizer(sm_summ_df['summary'].tolist(), return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "print(\"\\nSummarization Data Sizes\")\n",
    "print(f\"Text Input IDs: {text_inputs['input_ids'].size()}\")\n",
    "print(f\"Text Attention Mask: {text_inputs['attention_mask'].size()}\")\n",
    "print(f\"Summary Labels: {summary_labels.size()}\")\n",
    "summarization_dataset = TensorDataset(text_inputs['input_ids'], text_inputs['attention_mask'], summary_labels)\n",
    "summarization_dataloader = DataLoader(summarization_dataset, batch_size=4096, shuffle=True)\n",
    "\n",
    "\n",
    "# Multiple choice dataset\n",
    "mcq_inputs = tokenizer(sm_mcq_df['question'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "mcq_labels = torch.tensor([1 if label == 'correct' else 0 for label in sm_mcq_df['cop'].tolist()])\n",
    "print(\"\\nMultiple Choice Data Sizes\")\n",
    "print(f\"Question Input IDs: {mcq_inputs['input_ids'].size()}\")\n",
    "print(f\"Question Attention Mask: {mcq_inputs['attention_mask'].size()}\")\n",
    "print(f\"Labels: {mcq_labels.size()}\")\n",
    "mcq_dataset = TensorDataset(mcq_inputs['input_ids'], mcq_inputs['attention_mask'], mcq_labels)\n",
    "mcq_dataloader = DataLoader(mcq_dataset, batch_size=4096, shuffle=True)\n",
    "\"\"\"\n",
    "\n",
    "# Inspecting the DataLoaders: 1 batch\n",
    "def inspect_dataloader(dataloader, name):\n",
    "    print(f\"\\n{name} DataLoader Inspection:\")\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        print(f\"Input IDs size: {input_ids.size()}\")\n",
    "        print(f\"Attention Mask size: {attention_mask.size()}\")\n",
    "        print(f\"Labels size: {labels.size()}\")\n",
    "        break  \n",
    "\n",
    "#inspect_dataloader(translation_dataloader, \"Translation\")\n",
    "inspect_dataloader(classification_dataloader, \"Sentiment Classification\")\n",
    "#inspect_dataloader(summarization_dataloader, \"Summarization\")\n",
    "#inspect_dataloader(mcq_dataloader, \"Multiple Choice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7001e862-1ad6-40f8-ac90-b4a54962e44c",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 24pt;\"><strong>Defining `BertCustomHead` and training</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc59b5-4c29-4edc-9100-4581646758c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class BertCustomHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the BertCustomHead module. \n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_classes, task_type='sequence_classification'):\n",
    "        super(BertCustomHead, self).__init__()\n",
    "        self.bert = BertModel(config)\n",
    "        self.task_type = task_type\n",
    "        self.heads = {\n",
    "            'sequence_classification': nn.Linear(config.hidden_size, num_classes),\n",
    "            'token_classification': nn.Linear(config.hidden_size, num_classes),\n",
    "            'multiple_choice': nn.Linear(config.hidden_size, 1)\n",
    "        }\n",
    "        self.loss_fns = {\n",
    "            'sequence_classification': nn.CrossEntropyLoss(),\n",
    "            'token_classification': nn.CrossEntropyLoss(),\n",
    "            'multiple_choice': nn.BCEWithLogitsLoss()\n",
    "        }\n",
    "        \n",
    "        if task_type not in self.heads:\n",
    "            raise ValueError(\"Invalid task type. Supported types: 'sequence_classification', 'token_classification', 'multiple_choice'\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, next_sentence_labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.heads[self.task_type](pooled_output)\n",
    "\n",
    "def train(model, dataloaders, optimizer, num_epochs=5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Training loop for `num_epochs` epochs.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    task_losses = {task_name: [] for task_name in dataloaders.keys()}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for name, dataloader in dataloaders.items():\n",
    "            for batch in dataloader:\n",
    "                # Move batch to device\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, attention_mask, labels = batch\n",
    "\n",
    "                # Determine the task type based on the name of the data loader\n",
    "                if name == 'translation':\n",
    "                    task_type = 'sequence_classification'\n",
    "                    labels = labels.view(-1)  # Flatten the labels for sequence classification\n",
    "                elif name == 'classification':\n",
    "                    task_type = 'sequence_classification'\n",
    "                elif name == 'summarization':\n",
    "                    task_type = 'token_classification'\n",
    "                elif name == 'multiple_choice':\n",
    "                    task_type = 'multiple_choice'\n",
    "                    # Ensure labels have the same batch size as logits\n",
    "                    labels = labels.float()\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid data loader name. Supported names: 'translation', 'classification', 'summarization', 'multiple_choice'\")    \n",
    "                # Forward pass\n",
    "                logits = model(input_ids, attention_mask)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = model.loss_fns[task_type](logits, labels)\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                task_losses[name].append(loss.item())\n",
    "                \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Total Loss: {total_loss}\")\n",
    "        for task_name, losses in task_losses.items():\n",
    "            print(f\"Task: {task_name}, Last Loss: {losses[-1]}\")\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate model, optimizer, and dataloaders\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "model = BertCustomHead(config, num_classes=2, task_type='sequence_classification')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "dataloaders = {'classification': classification_dataloader}\n",
    "# Train the model\n",
    "train(model, dataloaders, optimizer, num_epochs=5, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71c6bd5-bf4e-4740-ae3e-30ef2ad530fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
