{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf7b83d-6920-43ea-a431-924cb0614144",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 24pt;\"><strong>Data Manipulation</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d37043-1227-4fd9-a919-b5f489835308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8656ea-98dc-469d-9843-846b1c341e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/jovyan/pub/en-fr.csv'\n",
    "enfr_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa08cdf8-f532-4418-be86-6467dd192f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataframe:\n",
      "(22520376, 2)\n",
      "Cardinality of categorical features in the dataframe:\n",
      "en    20317468\n",
      "fr    21358854\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "enfr_df['en'] = enfr_df['en'].astype(str).fillna('')\n",
    "enfr_df['fr'] = enfr_df['fr'].astype(str).fillna('')\n",
    "# Dimension\n",
    "print(\"Dimensions of the dataframe:\")\n",
    "print(enfr_df.shape)\n",
    "# Cardinality\n",
    "print(\"Cardinality of categorical features in the dataframe:\")\n",
    "print(enfr_df.select_dtypes(include=\"object\").nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bebc995-8de8-451b-a86b-793ec3f4176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_enfr_df = enfr_df.sample(n=18949, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ae58716-08cd-4a12-a6e1-1ae54e2de2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_path = '/home/jovyan/pub/rt_reviews.csv'\n",
    "rt_df = pd.read_csv(rt_path, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a9256fb-bc09-4992-aa2a-7fc256a0be05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2027767/224889234.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  rt_df['Freshness'] = rt_df['Freshness'].replace({'fresh': 0, 'rotten': 1})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataframe:\n",
      "(480000, 2)\n",
      "Duplicates in the dataframe: 140284 (29.23%)\n",
      "Cardinality of categorical features in the dataframe:\n",
      "Review    339697\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "rt_df['Review'] = rt_df['Review'].astype(str).fillna('')\n",
    "rt_df['Freshness'] = rt_df['Freshness'].replace({'fresh': 0, 'rotten': 1})\n",
    "# Dimension\n",
    "print(\"Dimensions of the dataframe:\")\n",
    "print(rt_df.shape)\n",
    "# Duplicates \n",
    "duplicates_count = rt_df.duplicated().sum()\n",
    "duplicates_percentage = 100 * duplicates_count / len(rt_df)\n",
    "print(f\"Duplicates in the dataframe: {duplicates_count} ({duplicates_percentage:.2f}%)\")\n",
    "# Cardinality \n",
    "print(\"Cardinality of categorical features in the dataframe:\")\n",
    "print(rt_df.select_dtypes(include=\"object\").nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7230b5ae-7aed-4222-8d66-45fa0b548049",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_rt_df = rt_df.sample(n=18949, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4483cc5e-b0fd-494c-8739-6125a5a651ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/jovyan/pub/bs_train.parquet'\n",
    "summ_df = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "349b4e00-e9b4-4af8-900e-8a90f116c2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataframe:\n",
      "(18949, 3)\n",
      "Duplicates in the dataframe: 0 (0.00%)\n",
      "Cardinality of categorical features in the dataframe:\n",
      "text       18941\n",
      "summary    18949\n",
      "title      17106\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "summ_df['text'] = summ_df['text'].astype(str).fillna('')\n",
    "summ_df['summary'] = summ_df['summary'].astype(str).fillna('')\n",
    "summ_df['title'] = summ_df['title'].astype(str).fillna('')\n",
    "# Dimension\n",
    "print(\"Dimensions of the dataframe:\")\n",
    "print(summ_df.shape)\n",
    "# Duplicates \n",
    "duplicates_count = summ_df.duplicated().sum()\n",
    "duplicates_percentage = 100 * duplicates_count / len(summ_df)\n",
    "print(f\"Duplicates in the dataframe: {duplicates_count} ({duplicates_percentage:.2f}%)\")\n",
    "# Cardinality (for categorical features)\n",
    "print(\"Cardinality of categorical features in the dataframe:\")\n",
    "print(summ_df.select_dtypes(include=\"object\").nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40a57f44-faaa-465f-82d1-97d1185a2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_summ_df = summ_df.sample(n=18949, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "027503c4-dbe7-404b-9812-21a7e4e2328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('/home/jovyan/pub/mcq.json', lines=True)\n",
    "single_choice_data = data[data['choice_type'] == 'single']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c5d16a7-cb1a-425c-a685-8c35ea88fdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2027767/2684607933.py:4: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['3' '3' '2' ... '2' '1' '1']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  single_choice_data.loc[:, 'cop'] = single_choice_data['cop'].astype(str).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataframe:\n",
      "(120765, 9)\n",
      "Duplicates in the dataframe: 0 (0.00%)\n",
      "Cardinality of categorical features in the dataframe:\n",
      "question        120765\n",
      "exp             103814\n",
      "cop                  4\n",
      "opa              51427\n",
      "opb              53743\n",
      "opc              54864\n",
      "opd              56004\n",
      "subject_name        21\n",
      "topic_name        2307\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# dropping Nan rows \n",
    "single_choice_data.loc[:, 'question'] = single_choice_data['question'].astype(str).fillna('')\n",
    "single_choice_data.loc[:, 'exp'] = single_choice_data['exp'].astype(str).fillna('')\n",
    "single_choice_data.loc[:, 'cop'] = single_choice_data['cop'].astype(str).fillna('')\n",
    "single_choice_data.loc[:, 'opa'] = single_choice_data['opa'].astype(str).fillna('')\n",
    "single_choice_data.loc[:, 'opb'] = single_choice_data['opb'].astype(str).fillna('')\n",
    "single_choice_data.loc[:, 'opc'] = single_choice_data['opc'].astype(str).fillna('')\n",
    "single_choice_data.loc[:, 'opd'] = single_choice_data['opd'].astype(str).fillna('')\n",
    "single_choice_data.loc[:, 'subject_name'] = single_choice_data['subject_name'].astype(str).fillna('')\n",
    "single_choice_data.loc[:, 'topic_name'] = single_choice_data['topic_name'].astype(str).fillna('')\n",
    "# dropping redundant cols\n",
    "single_choice_data = single_choice_data.drop(columns=['choice_type'])\n",
    "single_choice_data = single_choice_data.drop(columns=['id'])\n",
    "# Basic Sanity checks\n",
    "# Dimension\n",
    "print(\"Dimensions of the dataframe:\")\n",
    "print(single_choice_data.shape)\n",
    "# Duplicates \n",
    "duplicates_count = single_choice_data.duplicated().sum()\n",
    "duplicates_percentage = 100 * duplicates_count / len(single_choice_data)\n",
    "print(f\"Duplicates in the dataframe: {duplicates_count} ({duplicates_percentage:.2f}%)\")\n",
    "# Cardinality (for categorical features)\n",
    "print(\"Cardinality of categorical features in the dataframe:\")\n",
    "print(single_choice_data.select_dtypes(include=\"object\").nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc3b00f4-d0a1-4845-a1df-1db4a6601651",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_single_choice_data = single_choice_data.sample(n=18949, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa609e8-e9ba-487a-a766-e2c81bd7956d",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 24pt;\"><strong>Creating DataLoaders</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de57296d-0b25-4a93-bc12-cac98abfde72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8419e62e-65c5-4f88-baad-8df6d50793bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation Data Sizes\n",
      "Input IDs: torch.Size([18949, 512])\n",
      "Attention Mask: torch.Size([18949, 512])\n",
      "Labels: torch.Size([18949, 512])\n",
      "\n",
      "Sentiment Classification Data Sizes\n",
      "Input IDs: torch.Size([18949, 116])\n",
      "Attention Mask: torch.Size([18949, 116])\n",
      "Labels: torch.Size([18949])\n",
      "\n",
      "Summarization Data Sizes\n",
      "Text Input IDs: torch.Size([18949, 512])\n",
      "Text Attention Mask: torch.Size([18949, 512])\n",
      "Summary Labels: torch.Size([18949, 512])\n",
      "\n",
      "Multiple Choice Data Sizes\n",
      "Question Input IDs: torch.Size([18949, 278])\n",
      "Question Attention Mask: torch.Size([18949, 278])\n",
      "Labels: torch.Size([18949])\n",
      "\n",
      "Translation DataLoader Inspection:\n",
      "Input IDs size: torch.Size([8, 512])\n",
      "Attention Mask size: torch.Size([8, 512])\n",
      "Labels size: torch.Size([8, 512])\n",
      "\n",
      "Sentiment Classification DataLoader Inspection:\n",
      "Input IDs size: torch.Size([8, 116])\n",
      "Attention Mask size: torch.Size([8, 116])\n",
      "Labels size: torch.Size([8])\n",
      "\n",
      "Summarization DataLoader Inspection:\n",
      "Input IDs size: torch.Size([8, 512])\n",
      "Attention Mask size: torch.Size([8, 512])\n",
      "Labels size: torch.Size([8, 512])\n",
      "\n",
      "Multiple Choice DataLoader Inspection:\n",
      "Input IDs size: torch.Size([8, 278])\n",
      "Attention Mask size: torch.Size([8, 278])\n",
      "Labels size: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Sampling small subset of the dataset\n",
    "sm_enfr_df = enfr_df.sample(n=18949, random_state=42)\n",
    "sm_rt_df = rt_df.sample(n=18949, random_state=42)\n",
    "sm_summ_df = summ_df.sample(n=18949, random_state=42) \n",
    "sm_mcq_df = single_choice_data.sample(n=18949, random_state=42) \n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Trranslation dataset\n",
    "enfr_inputs = tokenizer(sm_enfr_df['en'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "enfr_labels = tokenizer(sm_enfr_df['fr'].tolist(), return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "print(\"Translation Data Sizes\")\n",
    "print(f\"Input IDs: {enfr_inputs['input_ids'].size()}\")\n",
    "print(f\"Attention Mask: {enfr_inputs['attention_mask'].size()}\")\n",
    "print(f\"Labels: {enfr_labels.size()}\")\n",
    "translation_dataset = TensorDataset(enfr_inputs['input_ids'], enfr_inputs['attention_mask'], enfr_labels)\n",
    "translation_dataloader = DataLoader(translation_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Sentiment classification dataset\n",
    "rt_inputs = tokenizer(sm_rt_df['Review'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "rt_labels = torch.tensor([1 if label == 'fresh' else 0 for label in sm_rt_df['Freshness'].tolist()])\n",
    "print(\"\\nSentiment Classification Data Sizes\")\n",
    "print(f\"Input IDs: {rt_inputs['input_ids'].size()}\")\n",
    "print(f\"Attention Mask: {rt_inputs['attention_mask'].size()}\")\n",
    "print(f\"Labels: {rt_labels.size()}\")\n",
    "classification_dataset = TensorDataset(rt_inputs['input_ids'], rt_inputs['attention_mask'], rt_labels)\n",
    "classification_dataloader = DataLoader(classification_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Text Summarization dataset\n",
    "text_inputs = tokenizer(sm_summ_df['text'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "summary_labels = tokenizer(sm_summ_df['summary'].tolist(), return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "print(\"\\nSummarization Data Sizes\")\n",
    "print(f\"Text Input IDs: {text_inputs['input_ids'].size()}\")\n",
    "print(f\"Text Attention Mask: {text_inputs['attention_mask'].size()}\")\n",
    "print(f\"Summary Labels: {summary_labels.size()}\")\n",
    "summarization_dataset = TensorDataset(text_inputs['input_ids'], text_inputs['attention_mask'], summary_labels)\n",
    "summarization_dataloader = DataLoader(summarization_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Multiple choice dataset\n",
    "mcq_inputs = tokenizer(sm_mcq_df['question'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "mcq_labels = torch.tensor([1 if label == 'correct' else 0 for label in sm_mcq_df['cop'].tolist()])\n",
    "print(\"\\nMultiple Choice Data Sizes\")\n",
    "print(f\"Question Input IDs: {mcq_inputs['input_ids'].size()}\")\n",
    "print(f\"Question Attention Mask: {mcq_inputs['attention_mask'].size()}\")\n",
    "print(f\"Labels: {mcq_labels.size()}\")\n",
    "mcq_dataset = TensorDataset(mcq_inputs['input_ids'], mcq_inputs['attention_mask'], mcq_labels)\n",
    "mcq_dataloader = DataLoader(mcq_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Inspecting the DataLoaders: 1 batch\n",
    "def inspect_dataloader(dataloader, name):\n",
    "    print(f\"\\n{name} DataLoader Inspection:\")\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        print(f\"Input IDs size: {input_ids.size()}\")\n",
    "        print(f\"Attention Mask size: {attention_mask.size()}\")\n",
    "        print(f\"Labels size: {labels.size()}\")\n",
    "        break  \n",
    "\n",
    "inspect_dataloader(translation_dataloader, \"Translation\")\n",
    "inspect_dataloader(classification_dataloader, \"Sentiment Classification\")\n",
    "inspect_dataloader(summarization_dataloader, \"Summarization\")\n",
    "inspect_dataloader(mcq_dataloader, \"Multiple Choice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7001e862-1ad6-40f8-ac90-b4a54962e44c",
   "metadata": {},
   "source": [
    "<p><span style=\"font-size: 24pt;\"><strong>Defining `BertCustomHead` and training</strong></span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29fe8f74-f810-4d9b-aae0-243c7289baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, BertOnlyNSPHead\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b30745d8-be86-40b7-8971-d969ead8fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertCustomHead(nn.Module):\n",
    "    def __init__(self, config, num_classes, task_type='sequence_classification'):\n",
    "        super(BertCustomHead, self).__init__()\n",
    "        self.bert = BertModel(config)\n",
    "        self.task_type = task_type\n",
    "        \n",
    "        if self.task_type == 'sequence_classification':\n",
    "            self.classifier = nn.Linear(config.hidden_size, num_classes)\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "        elif self.task_type == 'token_classification':\n",
    "            self.classifier = nn.Linear(config.hidden_size, num_classes)\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "        elif self.task_type == 'multiple_choice':\n",
    "            self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "            self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        elif self.task_type == 'next_sentence_prediction':\n",
    "            self.nsp_head = BertOnlyNSPHead(config)\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task type. Supported types: 'sequence_classification', 'token_classification', 'multiple_choice', 'next_sentence_prediction'\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, next_sentence_labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        if self.task_type == 'next_sentence_prediction':\n",
    "            nsp_logits = self.nsp_head(pooled_output)\n",
    "            return nsp_logits\n",
    "        else:\n",
    "            logits = self.classifier(pooled_output)\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3667fd7d-b4dc-4d89-bbc2-44394b6c19ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "model = BertCustomHead(config, num_classes=2, task_type='sequence_classification')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af2a505c-8fe0-4465-8e06-557591d429ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertCustomHead(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc51c6d-e0c8-465d-a4cb-0577e683d499",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mnum_epochs\u001b[49m):\n\u001b[1;32m      2\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, dataloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiple_choice\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      4\u001b[0m                                 [translation_dataloader, classification_dataloader, summarization_dataloader, mcq_dataloader]):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for name, dataloader in zip(['translation', 'classification', 'summarization', 'multiple_choice'],\n",
    "                                [translation_dataloader, classification_dataloader, summarization_dataloader, mcq_dataloader]):\n",
    "        for batch in dataloader:\n",
    "            # Move batch to device\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, attention_mask, labels = batch\n",
    "\n",
    "            # Determine the task type based on the name of the data loader\n",
    "            if name == 'translation':\n",
    "                task_type = 'sequence_classification'\n",
    "                labels = labels.view(-1)  # Flatten the labels for sequence classification\n",
    "            elif name == 'classification':\n",
    "                task_type = 'sequence_classification'\n",
    "            elif name == 'summarization':\n",
    "                task_type = 'token_classification'\n",
    "            elif name == 'multiple_choice':\n",
    "                task_type = 'multiple_choice'\n",
    "                # Ensure labels have the same batch size as logits\n",
    "                labels = labels.unsqueeze(1).float()  # Add a dimension to match logits\n",
    "                labels = labels.expand(-1, logits.size(1))  # Expand labels to match logits shape\n",
    "            else:\n",
    "                raise ValueError(\"Invalid data loader name. Supported names: 'translation', 'classification', 'summarization', 'multiple_choice'\")\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(input_ids, attention_mask)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = model.loss_fn(logits, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc59b5-4c29-4edc-9100-4581646758c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
