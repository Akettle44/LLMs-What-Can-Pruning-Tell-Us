{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook for Exploring Pruning of BERT: Inspired from: https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb#scrollTo=YOCrQwPoIrJG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task selected using Huggingface's Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/.pyenv/versions/3.10.2/envs/bertenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import datasets\n",
    "import transformers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task\n",
    "CoLA outputs a binary label of gramatically correct or not. It was deemed the easiest place to start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12341/887701111.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = datasets.load_metric(\"glue\", task)\n",
      "/home/andrew/.pyenv/versions/3.10.2/envs/bertenv/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup chosen task and metric\n",
    "task = \"cola\"\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "batch_size = 16\n",
    "\n",
    "dataset = datasets.load_dataset(\"glue\", task)\n",
    "metric = datasets.load_metric(\"glue\", task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 8551\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1043\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1063\n",
      "    })\n",
      "})\n",
      "{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\", 'label': 1, 'idx': 0}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric(name: \"glue\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
      "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
      "Args:\n",
      "    predictions: list of predictions to score.\n",
      "        Each translation should be tokenized into a list of tokens.\n",
      "    references: list of lists of references for each translation.\n",
      "        Each reference should be tokenized into a list of tokens.\n",
      "Returns: depending on the GLUE subset, one or several of:\n",
      "    \"accuracy\": Accuracy\n",
      "    \"f1\": F1 score\n",
      "    \"pearson\": Pearson Correlation\n",
      "    \"spearmanr\": Spearman Correlation\n",
      "    \"matthews_correlation\": Matthew Correlation\n",
      "Examples:\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'accuracy': 1.0}\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'accuracy': 1.0, 'f1': 1.0}\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
      "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
      "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
      "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'matthews_correlation': 1.0}\n",
      "\"\"\", stored examples: 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'matthews_correlation': 0.0640210337986158}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(metric)\n",
    "\n",
    "# Using metric\n",
    "fake_preds = np.random.randint(0, 2, size=(64,))\n",
    "fake_labels = np.random.randint(0, 2, size=(64,))\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7592, 1010, 2023, 2028, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/.pyenv/versions/3.10.2/envs/bertenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(checkpoint, use_fast=True)\n",
    "# Example\n",
    "print(tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\"))\n",
    "\n",
    "#print(f\"Sentence: {dataset['train'][0]['sentence']}\")\n",
    "\n",
    "# preprocess function\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['sentence'], truncation=True)\n",
    "\n",
    "preprocess_function(dataset['train'][:5])\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/.pyenv/versions/3.10.2/envs/bertenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Grab BERT for sequence classification\n",
    "\n",
    "num_labels = 2\n",
    "model = transformers.BertForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"matthews_correlation\"\n",
    "model_name = checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = transformers.TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_key = \"validation\"\n",
    "trainer = transformers.Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2675 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 503/2675 [00:23<01:40, 21.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4892, 'grad_norm': 8.492907524108887, 'learning_rate': 1.6261682242990654e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 533/2675 [00:24<01:27, 24.52it/s]\n",
      " 20%|██        | 535/2675 [00:25<01:27, 24.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44444724917411804, 'eval_matthews_correlation': 0.5272175286888164, 'eval_runtime': 0.6017, 'eval_samples_per_second': 1733.381, 'eval_steps_per_second': 109.687, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1004/2675 [00:49<01:14, 22.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2934, 'grad_norm': 7.86383581161499, 'learning_rate': 1.2523364485981309e-05, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1070/2675 [00:52<01:12, 22.08it/s]\n",
      " 40%|████      | 1070/2675 [00:53<01:12, 22.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47043007612228394, 'eval_matthews_correlation': 0.5932805322494611, 'eval_runtime': 0.6902, 'eval_samples_per_second': 1511.161, 'eval_steps_per_second': 95.625, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 1502/2675 [01:14<00:54, 21.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1912, 'grad_norm': 0.32137614488601685, 'learning_rate': 8.785046728971963e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 1604/2675 [01:19<00:40, 26.19it/s]\n",
      " 60%|██████    | 1605/2675 [01:19<00:40, 26.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6296312808990479, 'eval_matthews_correlation': 0.5680628967969402, 'eval_runtime': 0.5923, 'eval_samples_per_second': 1760.785, 'eval_steps_per_second': 111.421, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 2003/2675 [01:40<00:26, 25.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.138, 'grad_norm': 0.15411821007728577, 'learning_rate': 5.046728971962617e-06, 'epoch': 3.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 2138/2675 [01:46<00:25, 20.78it/s]\n",
      " 80%|████████  | 2140/2675 [01:47<00:25, 20.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9082818627357483, 'eval_matthews_correlation': 0.5572696682585848, 'eval_runtime': 0.5918, 'eval_samples_per_second': 1762.401, 'eval_steps_per_second': 111.523, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 2502/2675 [02:06<00:07, 21.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0874, 'grad_norm': 0.07196631282567978, 'learning_rate': 1.308411214953271e-06, 'epoch': 4.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 2673/2675 [02:14<00:00, 23.54it/s]\n",
      "100%|██████████| 2675/2675 [02:14<00:00, 23.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9003195762634277, 'eval_matthews_correlation': 0.5857509882742485, 'eval_runtime': 0.577, 'eval_samples_per_second': 1807.756, 'eval_steps_per_second': 114.393, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2675/2675 [02:17<00:00, 19.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 137.6442, 'train_samples_per_second': 310.62, 'train_steps_per_second': 19.434, 'train_loss': 0.22967105580267505, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2675, training_loss=0.22967105580267505, metrics={'train_runtime': 137.6442, 'train_samples_per_second': 310.62, 'train_steps_per_second': 19.434, 'total_flos': 454848611954580.0, 'train_loss': 0.22967105580267505, 'epoch': 5.0})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [00:00<00:00, 118.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.47043007612228394,\n",
       " 'eval_matthews_correlation': 0.5932805322494611,\n",
       " 'eval_runtime': 0.5726,\n",
       " 'eval_samples_per_second': 1821.65,\n",
       " 'eval_steps_per_second': 115.272,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_model(\"bert_pruning_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model == trained_model\n",
    "\n",
    "#for p1, p2 in zip(model.parameters(), trained_model.parameters()):\n",
    "#    if(p1.data.ne(p2.data).sum() > 0):\n",
    "#        print('False')\n",
    "#print('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "trained_model = transformers.BertForSequenceClassification.from_pretrained('bert_pruning_model', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sparisty of the network\n",
    "def calcSparsity(parameter_names):\n",
    "    zeros = 0\n",
    "    elements = 0\n",
    "\n",
    "    for name, param in trained_model.named_parameters():\n",
    "\n",
    "        # Layer was pruned if orig exists\n",
    "        #parsed_name = name.split('_orig')\n",
    "        # Layer was pruned\n",
    "        #if(len(parsed_name) == 2 and parsed_name[0] in parameter_names):\n",
    "\n",
    "        zero_count = torch.sum(param==0.0).item()\n",
    "        zeros += zero_count\n",
    "\n",
    "        element_count = param.numel()\n",
    "        elements += element_count\n",
    "\n",
    "    if(elements == 0):\n",
    "        return 0\n",
    "\n",
    "    return zeros / elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the parameters that you wish to prune\n",
    "parameters_to_prune = []\n",
    "parameter_names = []\n",
    "for name, module in trained_model.named_modules():\n",
    "\n",
    "    # Self-Attention\n",
    "    #if isinstance(module, transformers.models.bert.modeling_bert.BertSelfAttention):\n",
    "    #    parameters_to_prune.extend([\n",
    "    #            (module.query, 'weight'),\n",
    "    #            (module.key, 'weight'),\n",
    "    #            (module.value, 'weight'),\n",
    "    #        ])\n",
    "    #print(name)\n",
    "    #parameter_names.append(name)\n",
    "    #parameter_names.extend([name + '.query', name + '.key', name + '.value'])\n",
    "\n",
    "    # Linear: Includes linear layers in attention head as well\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        parameters_to_prune.append((module, 'weight'))\n",
    "        parameter_names.append(name + '.weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.7\n",
    ")\n",
    "\n",
    "for module,name in parameters_to_prune:\n",
    "    prune.remove(module, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5468226626231331"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcSparsity(parameter_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
